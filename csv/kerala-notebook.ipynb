{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import pickle\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest           #Used to identify most impactful features towards an outcome\n",
    "from sklearn.feature_selection import f_classif             #Score function for SelectKBest\n",
    "#Models\n",
    "from xgboost import XGBClassifier                           #https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics                                 #Provide accuracy and confidence scores & confusion matrix (see output template below) | https://wiki.pathmind.com/accuracy-precision-recall-f1\n",
    "#Confusion Matrix Output:\n",
    "# [True positives, False negatives]\n",
    "# [False positives ,True negatives]\n",
    "\n",
    "from lime import lime_tabular                               #Used to explain model results (i.e. show WHY a prediction was made as it had been)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\maf74\\\\source\\\\repos\\\\miked-generic-ml\\\\data\\\\./kerala.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Demo flood data | dataset is used to determine the most common months for flooding based on historical data for past 100+ years\n",
    "#Note: The models score well - often with 100% accuracy and confidence.\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "sourceType = \"csv\"\n",
    "sourceLocation = glob.glob(os.path.join(data_folder,\"./kerala.csv\"))\n",
    "colOutcome = \"FLOODS\"\n",
    "display(sourceLocation[0])\n",
    "colDrops = [\"SUBDIVISION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with FLOODS:    60\n",
      "Entries without FLOODS: 58\n",
      "Feature columns and data types:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 118 entries, 0 to 117\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   YEAR              118 non-null    int64  \n",
      " 1   JAN               118 non-null    float32\n",
      " 2   FEB               118 non-null    float32\n",
      " 3   MAR               118 non-null    float32\n",
      " 4   APR               118 non-null    float32\n",
      " 5   MAY               118 non-null    float32\n",
      " 6   JUN               118 non-null    float32\n",
      " 7   JUL               118 non-null    float32\n",
      " 8   AUG               118 non-null    float32\n",
      " 9   SEP               118 non-null    float32\n",
      " 10  OCT               118 non-null    float32\n",
      " 11  NOV               118 non-null    float32\n",
      " 12  DEC               118 non-null    float32\n",
      " 13   ANNUAL RAINFALL  118 non-null    float32\n",
      "dtypes: float32(13), int64(1)\n",
      "memory usage: 7.0 KB\n",
      "None\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(sourceLocation[0])        #Read file from source\n",
    "df = df.drop(colDrops, axis=1)          #Drop any columns noted as not needed\n",
    "Y = df[colOutcome]                      #Create series from this dataframe column (outcomes)\n",
    "\n",
    "#Perform data cleanup (str to int if needed) and set X (features) dataframe\n",
    "listNumCols = [col for col in df.columns if df[col].dtype == 'int64' or         #Get list of all int/float columns.  These do not need to be changed\n",
    "               df[col].dtype == 'float64']\n",
    "listOtherCols = [col for col in df[df.columns.difference(listNumCols)]]         #Get listing of all other columns.  These will be mapped to int representing str value\n",
    "for col in listOtherCols:                                                       #Loop through column list\n",
    "    mapper = {k: 0 + i for i, k in enumerate(set(df[col]))}                     #Create mapping object of unique values\n",
    "    df[col] = df[col].map(mapper)                                               #Apply mapped values back to original dataframe to encode character data\n",
    "float64_cols = list(df.select_dtypes(include='float64'))                        #Select columns with 'float64' dtype (convert tp 32 on line below.  needed fpr SKLearn)\n",
    "df[float64_cols] = df[float64_cols].astype('float32')                           #The same code again calling the columns\n",
    "\n",
    "X = df.drop([colOutcome], axis=1)                                               #Create features frame (also remove column noting outcome)\n",
    "print(\"Entries with \"+colOutcome+\":    \" + str(len(df[df[colOutcome] == 1])))   #Number of items indicating positive outcome\n",
    "print(\"Entries without \"+colOutcome+\": \" + str(len(df[df[colOutcome] == 0])))   #Number of items indicating negative outcome\n",
    "print(\"Feature columns and data types:\")\n",
    "print(X.info())\n",
    "print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance of features to outcome (lowest to highest):\n",
      "            Features       Score\n",
      "4                APR    0.137368\n",
      "2                FEB    0.148280\n",
      "12               DEC    0.343369\n",
      "10               OCT    0.420337\n",
      "3                MAR    0.879362\n",
      "1                JAN    2.502272\n",
      "0               YEAR    4.962598\n",
      "11               NOV    7.018248\n",
      "5                MAY    7.267663\n",
      "8                AUG   10.387935\n",
      "9                SEP   19.145486\n",
      "6                JUN   28.248747\n",
      "7                JUL   28.287433\n",
      "13   ANNUAL RAINFALL  175.849458\n"
     ]
    }
   ],
   "source": [
    "#Determine the features potentially most impacting\n",
    "bestFeatures = SelectKBest(score_func=f_classif, k=3).fit(X, Y)                 #https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "df_scores = pd.DataFrame(bestFeatures.scores_)                                  #This section will determine which features appear to have the most impact in the outcome\n",
    "df_columns = pd.DataFrame(X.columns)                                            # Note that the score_func value can be changed (see link above) to change the methodology\n",
    "featuresScores = pd.concat([df_columns, df_scores], axis=1)                     # and sampling of the data to determine impact of features.\n",
    "featuresScores.columns = [\"Features\", \"Score\"]\n",
    "print(\"Relevance of features to outcome (lowest to highest):\")\n",
    "print(featuresScores.sort_values(by=\"Score\"))                                   #Print listing of features sorted by their impact (greatest at bottom)\n",
    "dfMax = featuresScores[featuresScores.Score == featuresScores.Score.max()]      #Get row with highest scored feature\n",
    "maxFeature = dfMax[\"Features\"].iloc[0]                                          #Get name of highest scored feature\n",
    "\n",
    "#Split into train test sets (20% set aside to test the model)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)        #Create test & train sets with set test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy:  96.0 %\n",
      "Confidence Score (F1):  97.0 %\n",
      "Confusion Matrix Below:\n",
      "[[ 7  1]\n",
      " [ 0 16]]\n"
     ]
    }
   ],
   "source": [
    "modelRF = RandomForestClassifier(random_state=1, n_estimators=100)              #Create model\n",
    "modelRF.fit(X_train, Y_train)                                                   #Train model on data\n",
    "#Make and evaluate predictions\n",
    "modelPredictionsRF = modelRF.predict(X_test)                                    #Test model and record outcomes\n",
    "accRF = metrics.accuracy_score(Y_test, modelPredictionsRF)                      #Check model's accuracy\n",
    "print(\"Random Forest Accuracy: \", str(round(accRF, 2)*100), \"%\")\n",
    "\n",
    "#Confusion matrix identifies results outcomes by true/false positive/negative\n",
    "cm_modelRF = metrics.confusion_matrix(Y_test, modelPredictionsRF)               #Build confusion matrix for model\n",
    "F1_scoreRF = metrics.f1_score(Y_test, modelPredictionsRF)                       #Determine confidence in model\n",
    "print(\"Confidence Score (F1): \", str(round(F1_scoreRF, 2)*100), \"%\")\n",
    "print(\"Confusion Matrix Below:\")\n",
    "print(cm_modelRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy:  96.0 %\n",
      "Confidence Score (F1):  97.0 %\n",
      "Confusion Matrix Below:\n",
      "[[ 7  1]\n",
      " [ 0 16]]\n"
     ]
    }
   ],
   "source": [
    "modelXGB = XGBClassifier()                                                      #Create model\n",
    "modelXGB.fit(X_train, Y_train)                                                  #Train model on data\n",
    "#Make and evaluate predictions\n",
    "modelPredictionsXGB = modelXGB.predict(X_test)                                  #Test model and record outcomes\n",
    "accXGB = metrics.accuracy_score(Y_test, modelPredictionsXGB)                    #Check model's accuracy\n",
    "print(\"XGBoost Accuracy: \", str(round(accXGB, 2)*100), \"%\")\n",
    "\n",
    "#Confusion matrix identifies results outcomes by true/false positive/negative\n",
    "cm_modelXGB = metrics.confusion_matrix(Y_test, modelPredictionsXGB)             #Build confusion matrix for model\n",
    "F1_scoreXGB = metrics.f1_score(Y_test, modelPredictionsXGB)                     #Determine confidence in model\n",
    "print(\"Confidence Score (F1): \", str(round(F1_scoreXGB, 2)*100), \"%\")\n",
    "print(\"Confusion Matrix Below:\")\n",
    "print(cm_modelXGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirModels = os.path.join(os.getcwd(), 'models')\n",
    "# modelPath = os.path.join(dirModels,\"./DemoML_\" + colOutcome.replace(\" \", \"_\") + \".pymod\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "pickle.dump(modelXGB, open(\"outputs/kerala.pymod\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    YEAR        JAN        FEB        MAR        APR         MAY         JUN  \\\n",
      "62  1963  30.200001  24.799999  69.800003  96.300003  157.100006  393.299988   \n",
      "\n",
      "           JUL    AUG         SEP         OCT        NOV        DEC  \\\n",
      "62  720.200012  511.0  223.899994  282.600006  93.400002  48.400002   \n",
      "\n",
      "     ANNUAL RAINFALL  \n",
      "62       2651.100098  \n",
      "    YEAR        JAN        FEB        MAR        APR         MAY         JUN  \\\n",
      "62  1963  30.200001  24.799999  69.800003  96.300003  157.100006  393.299988   \n",
      "\n",
      "           JUL    AUG         SEP         OCT        NOV        DEC  \\\n",
      "62  720.200012  511.0  223.899994  282.600006  93.400002  48.400002   \n",
      "\n",
      "     ANNUAL RAINFALL  FLOODS  Prediction  Probability  Model Accuracy  \\\n",
      "62       2651.100098       0           0    98.000002           100.0   \n",
      "\n",
      "    Model Confidence  \n",
      "62             100.0  \n",
      "*****************************************************************\n",
      "                                 Feature     Value\n",
      "0  2603.15 <  ANNUAL RAINFALL <= 2934.30 -0.654849\n",
      "1                          JUN <= 514.02  0.020604\n",
      "2                 217.75 < OCT <= 287.45  0.018679\n",
      "3                 156.77 < SEP <= 224.45  0.017845\n",
      "4                 708.45 < JUL <= 834.87 -0.014773\n",
      "5                 125.05 < MAY <= 184.60 -0.012126\n",
      "6                            FEB > 22.53 -0.011235\n",
      "7                  70.52 < APR <= 111.60 -0.009529\n",
      "8                            JAN > 18.75  0.008332\n",
      "9                  92.60 < NOV <= 139.60 -0.003386\n"
     ]
    }
   ],
   "source": [
    "#Simulate \"new\" data to run through model (which has now been built and saved).  Below will load model and display results including prediction, probability and reason why\n",
    "dfS = df.sample(n=1)                                                                                        #Using original dataframe, randomly select rows (simulates \"new\" data to run through model)\n",
    "dfN = dfS.drop([colOutcome], axis=1, errors='ignore')                                                       #Drop stated outcome column as that wasn't in model's data and shouldn't be in \"new\" data\n",
    "modelXGB = pickle.load(open(\"outputs/kerala.pymod\", 'rb'))         #Recreate model based on saved file\n",
    "pred = modelXGB.predict(dfN)                                                                                #Make prediction on data using model (drop outcome column if it exists)\n",
    "prob = modelXGB.predict_proba(dfN)                                                                          #Make probability on prediction [Neg Pos] (drop outcome column if it exists)\n",
    "\n",
    "#Add columns to dataset to summarize model's prediction\n",
    "listProb = []                                                                                               #Create list to hold probability\n",
    "for i, p in enumerate(pred):                                                                                #Loop through predictions [%False %True]\n",
    "    listProb.append(round(prob[i][p], 2)*100)                                                               #Get probability for corresponding outcome (either false or true)\n",
    "\n",
    "dfS[\"Prediction\"] = pred.tolist()                                                                           #Apply prediction back to original data\n",
    "dfS[\"Probability\"] = listProb                                                                               #Apply probability back to original data\n",
    "dfS[\"Model Accuracy\"] = round(accXGB, 2)*100                                                                #Apply model accuracy\n",
    "dfS[\"Model Confidence\"] = round(F1_scoreXGB, 2)*100                                                         #Apply model confidence\n",
    "print(dfS)                                                                                                  #View sampled data\n",
    "\n",
    "print(\"*****************************************************************\")\n",
    "#Explain results (graphed and saved as HTML)\n",
    "explainer = lime_tabular.LimeTabularExplainer(X_train.to_numpy(), mode=\"regression\", feature_names=dfN.columns.values.tolist(), feature_selection=\"auto\")\n",
    "explanation = explainer.explain_instance(dfN.values.flatten(), modelXGB.predict)\n",
    "dfE = pd.DataFrame(explanation.as_list(label=1), columns=['Feature', 'Value'])\n",
    "print(dfE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new section, simply want to infer from everything above\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
